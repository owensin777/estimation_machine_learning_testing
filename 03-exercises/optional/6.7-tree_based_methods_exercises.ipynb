{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4e942e8-36ee-4adf-9bf5-6644173e8403",
   "metadata": {},
   "source": [
    "# 6.7: Tree-Based Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370bb0e1-530e-458d-a018-365f58495ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and specific objects\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "import sklearn.model_selection as skm\n",
    "from ISLP import load_data, confusion_table\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                         summarize)\n",
    "from sklearn.tree import (DecisionTreeClassifier as DTC,\n",
    "                          DecisionTreeRegressor as DTR,\n",
    "                          plot_tree,\n",
    "                          export_text)\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             log_loss)\n",
    "from sklearn.ensemble import \\\n",
    "     (RandomForestRegressor as RF,\n",
    "      GradientBoostingRegressor as GBR)\n",
    "from ISLP.bart import BART\n",
    "\n",
    "import warnings\n",
    "# Suppress FutureWarning in ISLP.models.columns\n",
    "# The warning is related to Series.__getitem__ treating keys as positions, which is deprecated.\n",
    "# Since ISLP is an external library that I don't control, and this specific warning does not\n",
    "# affect my current usage, I'm suppressing it to keep the output clean and focused on relevant information.\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning, module='ISLP.models.columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64209f76-46db-4449-99a4-df591b1eee61",
   "metadata": {},
   "source": [
    "### Fitting Classification Trees\n",
    "\n",
    "We first use classification trees to analyze the `Carseats` data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0fbf52-9dce-4753-9f1f-d6ea6f186957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "Carseats = load_data('Carseats')\n",
    "High = np.where(Carseats.Sales > 8,\n",
    "                \"Yes\",\n",
    "                \"No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e95cf1-ea6b-4a25-b5f2-dc07d1af2576",
   "metadata": {},
   "source": [
    "We now use `DecisionTreeClassifier()` (imported as `DTC`) to fit a classification tree in order to predict High using all variables but Sales. To do so, we must form a model matrix as we did when fitting regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b5ce2-e028-4664-bb3f-b2e9ec03e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MS(Carseats.columns.drop('Sales'), intercept=False)\n",
    "D = model.fit_transform(Carseats)\n",
    "feature_names = list(D.columns)\n",
    "X = np.asarray(D)\n",
    "\n",
    "# Specify classifer \n",
    "clf = DTC(criterion='entropy',\n",
    "          max_depth=3,\n",
    "          random_state=0)        \n",
    "clf.fit(X, High)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d8a5ef-8537-455c-b9c1-02dc6023db90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = subplots(figsize=(12,12))[1]\n",
    "plot_tree(clf,\n",
    "          feature_names=feature_names,\n",
    "          ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c807737-9d19-4dd5-886d-7f701513f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the accuracy score\n",
    "accuracy_score(High, clf.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f983c-01a1-4d34-9b2d-644024ab39b6",
   "metadata": {},
   "source": [
    "With only the default arguments, the training error rate is 21%.\n",
    "\n",
    "Now we will try out cost complexity pruning to see if we can get a tree with a better test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f8eb9-f74d-472d-b2c1-3bdddc4eae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "(X_train,\n",
    " X_test,\n",
    " High_train,\n",
    " High_test) = skm.train_test_split(X,\n",
    "                                   High,\n",
    "                                   test_size=0.5,\n",
    "                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4cdbfa-c751-441f-8e7e-c6d465314482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first refit the full tree on the training set\n",
    "clf = DTC(criterion='entropy', random_state=0)\n",
    "clf.fit(X_train, High_train)\n",
    "accuracy_score(High_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabda3a4-a20b-4da2-9ec7-3e7f81fe91f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we use the cost_complexity_pruning_path() method of clf to extract cost-complexity values\n",
    "ccp_path = clf.cost_complexity_pruning_path(X_train, High_train)\n",
    "kfold = skm.KFold(10,\n",
    "                  random_state=1,\n",
    "                  shuffle=True)\n",
    "\n",
    "grid = skm.GridSearchCV(clf,\n",
    "                        {'ccp_alpha': ccp_path.ccp_alphas},\n",
    "                        refit=True,\n",
    "                        cv=kfold,\n",
    "                        scoring='accuracy')\n",
    "grid.fit(X_train, High_train)\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c50227-97ea-46b1-8b61-9a7e21b56cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the leaves\n",
    "ax = subplots(figsize=(12, 12))[1]\n",
    "best_ = grid.best_estimator_\n",
    "plot_tree(best_,\n",
    "          feature_names=feature_names,\n",
    "          ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f9ea02-0a33-4766-a3b5-47f757ff49ee",
   "metadata": {},
   "source": [
    "**Compute the test error rate of this pruned tree. How does the test error rate and the interpretability\n",
    "of this tree compare to the inital tree?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa1ca4-c9a9-42cd-8f3e-62305e1ab447",
   "metadata": {},
   "source": [
    "### Fitting Regression Trees\n",
    "\n",
    "We will be fitting a regression tree to predict the median value of houses `medv` in `Boston` suburbs based on the information in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6412d18f-b286-41dd-a12d-e01692ef8364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "Boston = load_data(\"Boston\")\n",
    "model = MS(Boston.columns.drop('medv'), intercept=False)\n",
    "D = model.fit_transform(Boston)\n",
    "feature_names = list(D.columns)\n",
    "X = np.asarray(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd3266-2c95-4c5f-a4aa-5f20b54e01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into test and training\n",
    "(X_train,\n",
    " X_test,\n",
    " y_train,\n",
    " y_test) = skm.train_test_split(X,\n",
    "                                Boston['medv'],\n",
    "                                test_size=0.3,\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466c33a-7b2a-48b5-8443-227fb6be6046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit regression tree\n",
    "reg = DTR(max_depth=3)\n",
    "reg.fit(X_train, y_train)\n",
    "ax = subplots(figsize=(12,12))[1]\n",
    "plot_tree(reg,\n",
    "          feature_names=feature_names,\n",
    "          ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6165e3c4-dced-4a06-8906-5044b5da784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross-validation function to see whether pruning the tree will improve performance\n",
    "ccp_path = reg.cost_complexity_pruning_path(X_train, y_train)\n",
    "kfold = skm.KFold(5,\n",
    "                  shuffle=True,\n",
    "                  random_state=10)\n",
    "grid = skm.GridSearchCV(reg,\n",
    "                        {'ccp_alpha': ccp_path.ccp_alphas},\n",
    "                        refit=True,\n",
    "                        cv=kfold,\n",
    "                        scoring='neg_mean_squared_error')\n",
    "G = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac9b6a-7d8e-432b-a855-0036799ec792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pruned tree to make predictions on the test set\n",
    "best_ = grid.best_estimator_\n",
    "np.mean((y_test - best_.predict(X_test))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e927d34-f89f-480b-9ecf-fa54bbbd31fe",
   "metadata": {},
   "source": [
    "The test set MSE associated with the regression tree is 28.07."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b89d595-f44f-4a2b-a4d1-9e918765ff13",
   "metadata": {},
   "source": [
    "### Bagging and Random Forests\n",
    "\n",
    "We will use bagging and random forests on the `Boston` data set, using the `RandomForestRegressor()` from the `sklearn.ensemble` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d283c-cb74-4c86-ae6a-5b8e96de2ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_boston = RF(max_features=X_train.shape[1], random_state=0) # max_features indicates that all 12 predictors should be considered \n",
    "bag_boston.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174083cc-e560-468d-b4c3-7a57b69e205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = subplots(figsize=(8,8))[1]\n",
    "y_hat_bag = bag_boston.predict(X_test)\n",
    "ax.scatter(y_hat_bag, y_test)\n",
    "np.mean((y_test - y_hat_bag)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fe9979-5f9a-4477-ab73-5deb95b74cb7",
   "metadata": {},
   "source": [
    "The test set MSE associated with the bagged regression tree is 14.63, about half that obtained using an optimally-pruned single tree. We could change the number of trees grown from the default of 100 by using the `n_estimators` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae39a57-85b2-4bd5-8ec8-2f5195507fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_boston = RF(max_features=X_train.shape[1],\n",
    "                n_estimators=500,\n",
    "                random_state=0).fit(X_train, y_train)\n",
    "y_hat_bag = bag_boston.predict(X_test)\n",
    "np.mean((y_test - y_hat_bag)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9399f-a3a2-4c15-b72a-6385071145bd",
   "metadata": {},
   "source": [
    "There is not much change. Growing a random forest proceeds in exactly the same way, except that we use a smaller value of the `max_features`. Lets change `max_features=6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94f3472-fa34-4477-8bc1-f1ab4d85461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_boston = RF(max_features=6,\n",
    "               random_state=0).fit(X_train, y_train)\n",
    "y_hat_RF = RF_boston.predict(X_test)\n",
    "np.mean((y_test - y_hat_RF)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60312915-2b70-480a-9b01-32334363a1f2",
   "metadata": {},
   "source": [
    "The test set MSE is 20.04; this indicates that random forests did somewhat worse than bagging in this case. Extracting the `feature_importances_ values` from the fitted model, we can view the importance of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b3f81-d3ea-4e60-9378-7cbd338d49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(\n",
    "    {'importance':RF_boston.feature_importances_},\n",
    "    index=feature_names)\n",
    "feature_imp.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc48808-2a91-4b90-b35a-3263e23485e8",
   "metadata": {},
   "source": [
    "**Which two variables are the most important when determining median house values in Boston\n",
    "suburbs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c88d423-ef91-4e59-b35e-8ec149934bee",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "Here we use `GradientBoostingRegressor()` from `sklearn.ensemble` to fit boosted regression trees to the Boston data set. For classification we would use `GradientBoostingClassifier()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0347ca3-0a89-4f36-b393-1226bcfd88b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_boston = GBR(n_estimators=5000,\n",
    "                   learning_rate=0.001,\n",
    "                   max_depth=3,\n",
    "                   random_state=0)\n",
    "boost_boston.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d209e-e5ee-4e35-b7a2-0a5c9f842620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted values\n",
    "test_error = np.zeros_like(boost_boston.train_score_)\n",
    "for idx, y_ in enumerate(boost_boston.staged_predict(X_test)):\n",
    "   test_error[idx] = np.mean((y_test - y_)**2)\n",
    "\n",
    "plot_idx = np.arange(boost_boston.train_score_.shape[0])\n",
    "ax = subplots(figsize=(8,8))[1]\n",
    "ax.plot(plot_idx,\n",
    "        boost_boston.train_score_,\n",
    "        'b',\n",
    "        label='Training')\n",
    "ax.plot(plot_idx,\n",
    "        test_error,\n",
    "        'r',\n",
    "        label='Test')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886f4417-57c0-48f6-b70b-63d8b215cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now use the boosted model to predict medv on the test set\n",
    "\n",
    "y_hat_boost = boost_boston.predict(X_test);\n",
    "np.mean((y_test - y_hat_boost)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a4a936-d8e7-42c7-9f37-d2ffce0ed638",
   "metadata": {},
   "source": [
    "The test MSE obtained is 14.48,\n",
    "similar to the test MSE for bagging. If we want to, we can\n",
    "perform boosting with a different value of the shrinkage parameter\n",
    "$\\lambda$ in  (8.10). The default value is 0.001, but\n",
    "this is easily modified.  Here we take $\\lambda=0.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9178fe-d9f9-4743-8dd3-7461b9359414",
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_boston = GBR(n_estimators=5000,\n",
    "                   learning_rate=0.2,\n",
    "                   max_depth=3,\n",
    "                   random_state=0)\n",
    "boost_boston.fit(X_train,\n",
    "                 y_train)\n",
    "y_hat_boost = boost_boston.predict(X_test);\n",
    "np.mean((y_test - y_hat_boost)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a9905-504c-49c4-8114-e31fe9f70301",
   "metadata": {},
   "source": [
    "In this case, using $\\lambda=0.2$ leads to a almost the same test MSE\n",
    "as when using $\\lambda=0.001$.\n",
    "\n",
    "**Try fitting a new boosted model to the training set using a higher value for shrinkage and\n",
    "compute the test MSE. Which shrinkage parameter (between the two) yields the model with\n",
    "the best test error?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64030586-11f7-4d8b-b6b7-6854c990d986",
   "metadata": {},
   "source": [
    "### Bayesian Additive Regression Trees\n",
    "\n",
    "In this section we demonstrate a Python implementation of BART found in the `ISLP.bart` package. We fit a model to the Boston housing data set. This `BART()` estimator is designed for quantitative outcome variables, though other implementations are available for fitting logistic and probit models to categorical outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ec7c4-d0a3-48f3-9ca9-7af758bc8bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_boston = BART(random_state=0, burnin=5, ndraw=15)\n",
    "bart_boston.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0293c18-017c-4ec8-84d7-3816f1076204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into test and training\n",
    "yhat_test = bart_boston.predict(X_test.astype(np.float32))\n",
    "np.mean((y_test - yhat_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506fb275-4423-40ee-bcea-8c7bdb39de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many times each variable appeared in the collection of trees\n",
    "var_inclusion = pd.Series(bart_boston.variable_inclusion_.mean(0),\n",
    "                               index=D.columns)\n",
    "var_inclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d1b6b2-4820-441d-81a4-22eafedaeadf",
   "metadata": {},
   "source": [
    "*These exercises were adapted from :* James, Gareth, et al. An Introduction to Statistical Learning: with Applications in Python, Springer, 2023."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
