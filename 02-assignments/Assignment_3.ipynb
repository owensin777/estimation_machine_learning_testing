{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc88d9c-1760-4e18-a6c6-3ed1fc003b4b",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784f307d-c76d-4cf9-95fc-76fc8008754e",
   "metadata": {},
   "source": [
    "This assignment is due the following Monday. It pertains to content taught in classes 7-9.\n",
    "\n",
    "This assignment should be completed in Python, and a PDF file should be submitted, containing both code and written answers. If you like, you may create your own Jupyter Notebook file from scratch, but it is likely easier to modify this one.\n",
    "\n",
    "As before, questions that require identification and/or interpretation will not penalized for brevity of response: if a question can be answered with 'yes/no', or a numeric value, you may simply state as much. If you incorporate code from the internet (which is not required and generally not advisable), please cite the source within your code (providing a URL is sufficient).\n",
    "\n",
    "If you like, you may collaborate with others in the class. If you choose to do so, please indicate with whom you have worked at the top of your PDF. Separate submissions are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e7084f",
   "metadata": {},
   "source": [
    "### Question 1: Regularization via best subset selection\n",
    "\n",
    "First, we'll use the `swiss` dataset, which is a built-in dataset in R, but can be added to Python. As always, start by reviewing a description of the dataset, by typing `swiss?` in the console.  To perform model selection via \"best subsets\", we will use the `regsubsets` function in the `leaps` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "swiss = sm.datasets.get_rdataset(\"swiss\")\n",
    "df = pd.DataFrame(swiss.data)\n",
    "\n",
    "# Explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45265c1c",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "_(i)_ What will be the size (number of observations) of each LOOCV training sample?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3d0b7c",
   "metadata": {},
   "source": [
    "\n",
    "_(ii)_ What will be the size (number of observations) of each LOOCV testing sample?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17236269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767857e",
   "metadata": {},
   "source": [
    "\n",
    "_(iii)_ How many \"folds\" (i.e., k) will our LOOCV model have?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd49ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb4f80",
   "metadata": {},
   "source": [
    "\n",
    "_(iv)_ Now, fit a linear model, with `Fertility` as the response variable, and all other variables as predictors. Use the `sm.OLS` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a407f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a08a7",
   "metadata": {},
   "source": [
    "_(v)_ Next, perform LOOCV, using the appropriate function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a5a5d",
   "metadata": {},
   "source": [
    "_(vi)_ What is the MSE for the LOOCV?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c462a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15332a",
   "metadata": {},
   "source": [
    "_(vii)_ Run the LOOCV for a second time (no need to repeat the code; simply, run your existing code in in v and vi again). Do you obtain different results? Why or why not?  \n",
    "\n",
    "_(viii)_ Manually compute MSE for the linear model (without LOOCV) that you fit with the `sm.OLS` function, in iv. (Hint: recall that MSE is defined as the sum of squared residuals, divided by n. You can \"look inside\" your linear model object to find residual values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c13a36",
   "metadata": {},
   "source": [
    "_(ix)_ Does the LOOCV-linear model, or the non-validated linear model, appear to have greater error? Why might this be the case?   \n",
    "\n",
    "Imagine that the `swiss` dataset has just announced a major new release, which will include data from all provinces of Europe (not just those in Switzerland), and records all the way to the present day (not just 1888).  \n",
    "\n",
    "_(x)_ Would you choose LOOCV as a validation method for this new release? Why or why not?  \n",
    "\n",
    "_(xi)_ What validation method might you choose instead?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefc97a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ddb4d3-f622-441f-90a9-46e2b7da479b",
   "metadata": {},
   "source": [
    "### Question 2: Regularization via Shrinkage\n",
    "\n",
    "For this assignment, we'll use the in-built dataset credit from ISLP library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee71f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import subplots\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Import specific objects\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence \\\n",
    "     import variance_inflation_factor as VIF\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "%pip install l0bnb\n",
    "from l0bnb import fit_path\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                         summarize,\n",
    "                         poly)\n",
    "\n",
    "# Install, import, and load specific package\n",
    "%pip install faraway > nul 2>&1 # \"> nul 2>&1\" means that the install messages have been surpressed\n",
    "import faraway as fw\n",
    "import faraway.datasets.fat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1386a368",
   "metadata": {},
   "source": [
    "It is very important to understand if our data has missing values, which R represents as NA. Below, show that there are 0 NA values in the dataset. (Hint: you can use the function np.isnan() to search for NA values, and wrap that with the sum() function, to provide a total count.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8222a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487834d",
   "metadata": {},
   "source": [
    "It is also very important to visualize our data before modeling. The sns.pairplot() function visualizes the pair-wise correlations between all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d9bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = my_df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Create a scatterplot matrix\n",
    "sns.set_style(\"ticks\")\n",
    "sns.pairplot(numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb14f16",
   "metadata": {},
   "source": [
    "For much of our modelling, we'll make use of a separate training and testing set. Choose your favourite method to split my_df into equally-sized training and testing sets. For clarity, call your training set train, and your testing set test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7534d978",
   "metadata": {},
   "source": [
    "\n",
    "Shrinkage methods can \"extend\" or improve upon linear model fits, by pushing coefficients towards (ridge regression) or to zero (lasso), and thus reducing variance. Let's perform ridge regression, using the `skl.ElasticNet()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb05183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6dd538-a321-41b1-9c33-987a29311abf",
   "metadata": {},
   "source": [
    "Use our `my_df` dataset (deriving from Credit). Let's use `Balance` as the response variable, and all other variables as predictors. \n",
    "\n",
    "_(i)_ A necessary first step is to get our data into the format expected. Specifically, we must provide predictor variables in a matrix, and the response variable in a vector. For clarity, call the predictor matrix `x`, and the response vector `y`. (Hint: your `x` matrix should have should 400 rows and 11 columns. Verify that this is true, using in-built functions of your choice).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72023490-4fc2-4740-ac03-393b0236645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of the predictor variables \n",
    "\n",
    "# create a vector of the response variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767e8a0-4e1f-4741-b4f2-ac56b74ea761",
   "metadata": {},
   "source": [
    "Let's check out how `ModelSpec()` has transformed our data. Compare the names of variables in our matrix `x`, compared to `my_df` (hint: use the `columns` function), and answer:\n",
    "\n",
    "_(ii)_ Which \"type\" of variables (numeric, character, factor, etc.) have a new name in `x`?  \n",
    "\n",
    "_(iii)_ Which variable in `x` has two columns dedicated to it? Why? \n",
    "\n",
    "_(iv)_ What variable in `my_df` is missing in x? Why might this be?    \n",
    "\n",
    "Now that we understand how our data is represented, we can move on to modelling. Fit a ridge regression model, using `skl.ElasticNet()`. (Hint: remember to set the alpha value!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac92b4fe-8ad6-47ae-86ee-ee58cca9e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47136155-4a1d-48a3-bbc9-14d78996c78e",
   "metadata": {},
   "source": [
    "_(v)_ An essential part of ridge regression (and shrinkage methods more broadly) is to identify an 'ideal' lambda value. Use the appropriate function from `sk.learn` to identify this lambda value via cross-validation. (Hint: remember that `x` and `y` should not consist of the complete dataset!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c41a7-fa92-45fd-b4ce-94c9b116b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c043154-6c23-4eea-87dd-2c0fffd78f4e",
   "metadata": {},
   "source": [
    "_(vi)_ By default, cross validation via `skl.ElasticNet()` considers n=100 lambda values. The cross-validated model object that you created in the step above stores these n=100 lambda values within it. Print them here (Hint: use the `$` to \"look inside\" your model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058a4d6-38f3-4b00-be5d-7f376ae80973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2547e82-ab98-4b6c-ba21-49c369293b34",
   "metadata": {},
   "source": [
    "_(vii)_ Visualize your cross-validation results using `plot`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d35608-cba4-40b6-bad9-946cbb08a745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c805e7-c735-4f78-b4ca-1226b4e5a127",
   "metadata": {},
   "source": [
    "_(viii)_ Now, look inside your cross-validated object to pull out the lambda value with the smallest error (Hint: the value will be that shown by the first, left-most vertical dotted line.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669aa75-70ac-46b7-9e7b-51b0619ea559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2133e2e7-8adc-4a95-8a3b-8f3621845f17",
   "metadata": {},
   "source": [
    "_(ix)_ In your plot, what does the second (right-most) vertical dotted represent? (Hint: read the help documentation pertaining to `l1_ratio=0`.). \n",
    "\n",
    "_(x)_ We can now refit ridge regression, for the entire dataset, with the ideal lambda value. Use the lambda value with the smallest error. Provide an argument to print the estimated coefficients (Hint: check out the `type` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5891b-1389-4f78-944c-d258cea5da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118d62d7-d98d-47ac-9a7d-3ad4eee43ac8",
   "metadata": {},
   "source": [
    "_(xi)_ Did you expect any coefficients to be exactly 0? Why or why not?  \n",
    "\n",
    "_(xii)_ The plot created above shows that the ideal 'tuning' (penalty) provided by lambda is comparatively small (one of the smallest considered by `skl.ElasticNet()`, if not the smallest). What might this suggest? In your answer, consider the nature of the `Credit` dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "497a84dc8fec8cf8d24e7e87b6d954c9a18a327edc66feb9b9ea7e9e72cc5c7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
