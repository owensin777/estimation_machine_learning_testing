{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e5f602d-0241-4f0e-9aaa-aedc110ae8d0",
   "metadata": {},
   "source": [
    "## DSI-06 Homework 3: ANSWERS\n",
    "From Chapter 4, found on pages 196-197 of ISLP\n",
    "\n",
    "*This question should be answered using the `Weekly` data set, which is part of the ISLP package. This data is similar in nature to the Smarket data from this section's in-class exercises, except that it contains 1089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af43c39d-5b12-4cd9-82a9-facb32c835de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-3.484</td>\n",
       "      <td>0.154976</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.148574</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>0.159837</td>\n",
       "      <td>3.514</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990</td>\n",
       "      <td>3.514</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>1.572</td>\n",
       "      <td>0.161630</td>\n",
       "      <td>0.712</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990</td>\n",
       "      <td>0.712</td>\n",
       "      <td>3.514</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>-0.270</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.153728</td>\n",
       "      <td>1.178</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>2010</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-2.173</td>\n",
       "      <td>3.599</td>\n",
       "      <td>0.015</td>\n",
       "      <td>3.205160</td>\n",
       "      <td>2.969</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>2010</td>\n",
       "      <td>2.969</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-2.173</td>\n",
       "      <td>3.599</td>\n",
       "      <td>4.242568</td>\n",
       "      <td>1.281</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>2010</td>\n",
       "      <td>1.281</td>\n",
       "      <td>2.969</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-2.173</td>\n",
       "      <td>4.835082</td>\n",
       "      <td>0.283</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>2010</td>\n",
       "      <td>0.283</td>\n",
       "      <td>1.281</td>\n",
       "      <td>2.969</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>0.043</td>\n",
       "      <td>4.454044</td>\n",
       "      <td>1.034</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>2010</td>\n",
       "      <td>1.034</td>\n",
       "      <td>0.283</td>\n",
       "      <td>1.281</td>\n",
       "      <td>2.969</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>2.707105</td>\n",
       "      <td>0.069</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1089 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year   Lag1   Lag2   Lag3   Lag4   Lag5    Volume  Today Direction\n",
       "0     1990  0.816  1.572 -3.936 -0.229 -3.484  0.154976 -0.270      Down\n",
       "1     1990 -0.270  0.816  1.572 -3.936 -0.229  0.148574 -2.576      Down\n",
       "2     1990 -2.576 -0.270  0.816  1.572 -3.936  0.159837  3.514        Up\n",
       "3     1990  3.514 -2.576 -0.270  0.816  1.572  0.161630  0.712        Up\n",
       "4     1990  0.712  3.514 -2.576 -0.270  0.816  0.153728  1.178        Up\n",
       "...    ...    ...    ...    ...    ...    ...       ...    ...       ...\n",
       "1084  2010 -0.861  0.043 -2.173  3.599  0.015  3.205160  2.969        Up\n",
       "1085  2010  2.969 -0.861  0.043 -2.173  3.599  4.242568  1.281        Up\n",
       "1086  2010  1.281  2.969 -0.861  0.043 -2.173  4.835082  0.283        Up\n",
       "1087  2010  0.283  1.281  2.969 -0.861  0.043  4.454044  1.034        Up\n",
       "1088  2010  1.034  0.283  1.281  2.969 -0.861  2.707105  0.069        Up\n",
       "\n",
       "[1089 rows x 9 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Import specific objects\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                         summarize)\n",
    "from ISLP import load_data\n",
    "from ISLP import confusion_table\n",
    "from ISLP.models import contrast\n",
    "from sklearn.discriminant_analysis import \\\n",
    "     (LinearDiscriminantAnalysis as LDA,\n",
    "      QuadraticDiscriminantAnalysis as QDA)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "Weekly = load_data('Weekly')\n",
    "Weekly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c5ccd47-ca85-4e62-b0e1-0b429f2730a8",
   "metadata": {},
   "source": [
    "a) Produce some numerical and graphical summaries of the `Weekly` data. Do there appear to be any patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f9f5185-9cb5-4da8-a51a-02a41f5048c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "      <td>1089.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2000.048669</td>\n",
       "      <td>0.150585</td>\n",
       "      <td>0.151079</td>\n",
       "      <td>0.147205</td>\n",
       "      <td>0.145818</td>\n",
       "      <td>0.139893</td>\n",
       "      <td>1.574618</td>\n",
       "      <td>0.149899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.033182</td>\n",
       "      <td>2.357013</td>\n",
       "      <td>2.357254</td>\n",
       "      <td>2.360502</td>\n",
       "      <td>2.360279</td>\n",
       "      <td>2.361285</td>\n",
       "      <td>1.686636</td>\n",
       "      <td>2.356927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1990.000000</td>\n",
       "      <td>-18.195000</td>\n",
       "      <td>-18.195000</td>\n",
       "      <td>-18.195000</td>\n",
       "      <td>-18.195000</td>\n",
       "      <td>-18.195000</td>\n",
       "      <td>0.087465</td>\n",
       "      <td>-18.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1995.000000</td>\n",
       "      <td>-1.154000</td>\n",
       "      <td>-1.154000</td>\n",
       "      <td>-1.158000</td>\n",
       "      <td>-1.158000</td>\n",
       "      <td>-1.166000</td>\n",
       "      <td>0.332022</td>\n",
       "      <td>-1.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>1.002680</td>\n",
       "      <td>0.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2005.000000</td>\n",
       "      <td>1.405000</td>\n",
       "      <td>1.409000</td>\n",
       "      <td>1.409000</td>\n",
       "      <td>1.409000</td>\n",
       "      <td>1.405000</td>\n",
       "      <td>2.053727</td>\n",
       "      <td>1.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2010.000000</td>\n",
       "      <td>12.026000</td>\n",
       "      <td>12.026000</td>\n",
       "      <td>12.026000</td>\n",
       "      <td>12.026000</td>\n",
       "      <td>12.026000</td>\n",
       "      <td>9.328214</td>\n",
       "      <td>12.026000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Year         Lag1         Lag2         Lag3         Lag4  \\\n",
       "count  1089.000000  1089.000000  1089.000000  1089.000000  1089.000000   \n",
       "mean   2000.048669     0.150585     0.151079     0.147205     0.145818   \n",
       "std       6.033182     2.357013     2.357254     2.360502     2.360279   \n",
       "min    1990.000000   -18.195000   -18.195000   -18.195000   -18.195000   \n",
       "25%    1995.000000    -1.154000    -1.154000    -1.158000    -1.158000   \n",
       "50%    2000.000000     0.241000     0.241000     0.241000     0.238000   \n",
       "75%    2005.000000     1.405000     1.409000     1.409000     1.409000   \n",
       "max    2010.000000    12.026000    12.026000    12.026000    12.026000   \n",
       "\n",
       "              Lag5       Volume        Today  \n",
       "count  1089.000000  1089.000000  1089.000000  \n",
       "mean      0.139893     1.574618     0.149899  \n",
       "std       2.361285     1.686636     2.356927  \n",
       "min     -18.195000     0.087465   -18.195000  \n",
       "25%      -1.166000     0.332022    -1.154000  \n",
       "50%       0.234000     1.002680     0.241000  \n",
       "75%       1.405000     2.053727     1.405000  \n",
       "max      12.026000     9.328214    12.026000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add your code here\n",
    "\n",
    "Weekly.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86001139-f7c5-43d9-b10a-93a59e810cd4",
   "metadata": {},
   "source": [
    "b) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Do not forget to drop missing values due to lag creation. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e034864-89b7-4854-a5a3-87d8450515bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:              Direction   No. Observations:                 1084\n",
      "Model:                            GLM   Df Residuals:                     1072\n",
      "Model Family:                Binomial   Df Model:                           11\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -733.41\n",
      "Date:                Tue, 26 Mar 2024   Deviance:                       1466.8\n",
      "Time:                        01:50:33   Pearson chi2:                 1.08e+03\n",
      "No. Iterations:                     4   Pseudo R-squ. (CS):            0.02064\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.6979      0.250      2.795      0.005       0.208       1.187\n",
      "Lag1           0.0072      0.038      0.187      0.852      -0.068       0.082\n",
      "Lag2           0.1120      0.040      2.766      0.006       0.033       0.191\n",
      "Lag3          -0.0278      0.040     -0.702      0.482      -0.106       0.050\n",
      "Lag4          -0.0504      0.039     -1.300      0.194      -0.126       0.026\n",
      "Lag5           0.0474      0.038      1.232      0.218      -0.028       0.123\n",
      "Volume        -0.0184      0.037     -0.493      0.622      -0.092       0.055\n",
      "Lag_1_Up      -0.3280      0.181     -1.814      0.070      -0.682       0.026\n",
      "Lag_2_Up      -0.3007      0.184     -1.634      0.102      -0.661       0.060\n",
      "Lag_3_Up       0.0774      0.183      0.423      0.673      -0.282       0.437\n",
      "Lag_4_Up       0.1446      0.181      0.799      0.424      -0.210       0.499\n",
      "Lag_5_Up      -0.4130      0.181     -2.282      0.022      -0.768      -0.058\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "#Add your code here\n",
    "\n",
    "# Create lag variables\n",
    "for i in range(1, 6):\n",
    "    Weekly[f'Lag_{i}'] = Weekly['Direction'].shift(i)\n",
    "\n",
    "# Drop rows with missing values due to lag creation\n",
    "Weekly = Weekly.dropna()\n",
    "\n",
    "# Select predictors and response variable\n",
    "allvars = Weekly.columns.drop(['Today', 'Direction', 'Year'])\n",
    "design = sm.add_constant(pd.get_dummies(Weekly[allvars], drop_first=True))\n",
    "X = design\n",
    "y = (Weekly['Direction'] == 'Up').astype(int)\n",
    "\n",
    "# Fit logistic regression model\n",
    "glm = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "results = glm.fit()\n",
    "\n",
    "# Print the summary of the logistic regression\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dcf794-503d-4ffd-8501-3b2b9afa9a3e",
   "metadata": {},
   "source": [
    "c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24fae233-67a4-4b81-ad9e-63efd6fbf582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0          129          353\n",
      "Actual 1          108          494\n",
      "\n",
      "Overall Fraction of Correct Predictions (Accuracy): 0.5747232472324724\n"
     ]
    }
   ],
   "source": [
    "#Add your code here\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_prob = results.predict()\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1)\n",
    "y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "# Extract values from the confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Compute overall fraction of correct predictions (accuracy)\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "# Print confusion matrix and accuracy\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(cm, columns=['Predicted 0', 'Predicted 1'], index=['Actual 0', 'Actual 1']))\n",
    "print(\"\\nOverall Fraction of Correct Predictions (Accuracy):\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e9220-055f-4135-80c4-f288f65e8978",
   "metadata": {},
   "source": [
    "d) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1aed944-4885-45ec-8f14-2725ccc2455c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "             Predicted Down  Predicted Up\n",
      "Actual Down               9            34\n",
      "Actual Up                 5            56\n",
      "\n",
      "Overall Fraction of Correct Predictions (Accuracy): 0.625\n"
     ]
    }
   ],
   "source": [
    "#Add your code here\n",
    "\n",
    "# Filter data for the training period (1990 to 2008)\n",
    "train_data = Weekly[(Weekly['Year'] >= 1990) & (Weekly['Year'] <= 2008)]\n",
    "\n",
    "# Filter data for the test period (2009 and 2010)\n",
    "test_data = Weekly[(Weekly['Year'] == 2009) | (Weekly['Year'] == 2010)]\n",
    "\n",
    "# Extract predictors and response variables for training\n",
    "X_train = train_data[['Lag2']]\n",
    "y_train = train_data['Direction']\n",
    "\n",
    "# Extract predictors and response variables for testing\n",
    "X_test = test_data[['Lag2']]\n",
    "y_test = test_data['Direction']\n",
    "\n",
    "# Fit logistic regression model on the training data\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted probabilities on the test data\n",
    "y_prob = logreg_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1)\n",
    "y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "# Convert labels in y_true to numeric values (0 and 1)\n",
    "y_true_numeric = y_test.map({'Down': 0, 'Up': 1})\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true_numeric, y_pred)\n",
    "\n",
    "# Extract values from the confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Compute overall fraction of correct predictions (accuracy)\n",
    "accuracy = accuracy_score(y_true_numeric, y_pred)\n",
    "\n",
    "# Print confusion matrix and accuracy\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(cm, columns=['Predicted Down', 'Predicted Up'], index=['Actual Down', 'Actual Up']))\n",
    "print(\"\\nOverall Fraction of Correct Predictions (Accuracy):\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e216ddf1-fe11-4828-91cd-602dcc99f110",
   "metadata": {},
   "source": [
    "e) Repeat (d) using LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19f61bb3-e580-44d8-bdf2-fd1df2956720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "             Predicted Down  Predicted Up\n",
      "Actual Down               9            34\n",
      "Actual Up                 5            56\n",
      "\n",
      "Overall Fraction of Correct Predictions (Accuracy): 0.625\n"
     ]
    }
   ],
   "source": [
    "#Add your code here\n",
    "\n",
    "# Filter data for the training period (1990 to 2008)\n",
    "train_data = Weekly[(Weekly['Year'] >= 1990) & (Weekly['Year'] <= 2008)]\n",
    "\n",
    "# Filter data for the test period (2009 and 2010)\n",
    "test_data = Weekly[(Weekly['Year'] == 2009) | (Weekly['Year'] == 2010)]\n",
    "\n",
    "# Extract predictors and response variables for training\n",
    "X_train = train_data[['Lag2']]\n",
    "y_train = train_data['Direction']\n",
    "\n",
    "# Extract predictors and response variables for testing\n",
    "X_test = test_data[['Lag2']]\n",
    "y_test = test_data['Direction']\n",
    "\n",
    "# Fit LDA model on the training data\n",
    "lda_model = LDA()\n",
    "lda_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test data\n",
    "y_pred = lda_model.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract values from the confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Compute overall fraction of correct predictions (accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print confusion matrix and accuracy\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(cm, columns=['Predicted Down', 'Predicted Up'], index=['Actual Down', 'Actual Up']))\n",
    "print(\"\\nOverall Fraction of Correct Predictions (Accuracy):\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b1aae8-9bd0-4d41-8bed-863ff0a2117f",
   "metadata": {},
   "source": [
    "f) Repeat (d) using QDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcf6063f-c52e-4eba-a930-8aaa4ed80c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "             Predicted Down  Predicted Up\n",
      "Actual Down               0            43\n",
      "Actual Up                 0            61\n",
      "\n",
      "Overall Fraction of Correct Predictions (Accuracy): 0.5865384615384616\n"
     ]
    }
   ],
   "source": [
    "#Add your code here\n",
    "\n",
    "# Filter data for the training period (1990 to 2008)\n",
    "train_data = Weekly[(Weekly['Year'] >= 1990) & (Weekly['Year'] <= 2008)]\n",
    "\n",
    "# Filter data for the test period (2009 and 2010)\n",
    "test_data = Weekly[(Weekly['Year'] == 2009) | (Weekly['Year'] == 2010)]\n",
    "\n",
    "# Extract predictors and response variables for training\n",
    "X_train = train_data[['Lag2']]\n",
    "y_train = train_data['Direction']\n",
    "\n",
    "# Extract predictors and response variables for testing\n",
    "X_test = test_data[['Lag2']]\n",
    "y_test = test_data['Direction']\n",
    "\n",
    "# Fit QDA model on the training data\n",
    "qda_model = QDA()\n",
    "qda_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test data\n",
    "y_pred = qda_model.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract values from the confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Compute overall fraction of correct predictions (accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print confusion matrix and accuracy\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(cm, columns=['Predicted Down', 'Predicted Up'], index=['Actual Down', 'Actual Up']))\n",
    "print(\"\\nOverall Fraction of Correct Predictions (Accuracy):\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd84667-3df1-4f84-b2c6-c8ded360c91b",
   "metadata": {},
   "source": [
    "g) Repeat (d) using KNN with K = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3abebebc-4052-4d28-a1cd-1e9c69011d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "             Predicted Down  Predicted Up\n",
      "Actual Down              22            21\n",
      "Actual Up                31            30\n",
      "\n",
      "Overall Fraction of Correct Predictions (Accuracy): 0.5\n"
     ]
    }
   ],
   "source": [
    "#Add your code here\n",
    "\n",
    "# Filter data for the training period (1990 to 2008)\n",
    "train_data = Weekly[(Weekly['Year'] >= 1990) & (Weekly['Year'] <= 2008)]\n",
    "\n",
    "# Filter data for the test period (2009 and 2010)\n",
    "test_data = Weekly[(Weekly['Year'] == 2009) | (Weekly['Year'] == 2010)]\n",
    "\n",
    "# Extract predictors and response variables for training\n",
    "X_train = train_data[['Lag2']]\n",
    "y_train = train_data['Direction']\n",
    "\n",
    "# Extract predictors and response variables for testing\n",
    "X_test = test_data[['Lag2']]\n",
    "y_test = test_data['Direction']\n",
    "\n",
    "# Fit KNN model on the training data with K=1\n",
    "knn_model = KNeighborsClassifier(n_neighbors=1)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test data\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract values from the confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Compute overall fraction of correct predictions (accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print confusion matrix and accuracy\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(cm, columns=['Predicted Down', 'Predicted Up'], index=['Actual Down', 'Actual Up']))\n",
    "print(\"\\nOverall Fraction of Correct Predictions (Accuracy):\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27d831-0188-4381-8deb-218ec062d4f1",
   "metadata": {},
   "source": [
    "h) Repeat (d) using naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99c27b5f-8296-4b07-b67d-6482ca851cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "             Predicted Down  Predicted Up\n",
      "Actual Down               0            43\n",
      "Actual Up                 0            61\n",
      "\n",
      "Overall Fraction of Correct Predictions (Accuracy): 0.5865384615384616\n"
     ]
    }
   ],
   "source": [
    "#Add your code here\n",
    "\n",
    "# Filter data for the training period (1990 to 2008)\n",
    "train_data = Weekly[(Weekly['Year'] >= 1990) & (Weekly['Year'] <= 2008)]\n",
    "\n",
    "# Filter data for the test period (2009 and 2010)\n",
    "test_data = Weekly[(Weekly['Year'] == 2009) | (Weekly['Year'] == 2010)]\n",
    "\n",
    "# Extract predictors and response variables for training\n",
    "X_train = train_data[['Lag2']]\n",
    "y_train = train_data['Direction']\n",
    "\n",
    "# Extract predictors and response variables for testing\n",
    "X_test = test_data[['Lag2']]\n",
    "y_test = test_data['Direction']\n",
    "\n",
    "# Fit Naive Bayes model on the training data\n",
    "naive_bayes_model = GaussianNB()\n",
    "naive_bayes_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test data\n",
    "y_pred = naive_bayes_model.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract values from the confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Compute overall fraction of correct predictions (accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print confusion matrix and accuracy\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(cm, columns=['Predicted Down', 'Predicted Up'], index=['Actual Down', 'Actual Up']))\n",
    "print(\"\\nOverall Fraction of Correct Predictions (Accuracy):\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2fdba9-6143-41c7-a3f6-d147170192f9",
   "metadata": {},
   "source": [
    "i) Which of these methods appears to provide the best results on this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ceabd-ddaf-4779-b6f9-cb604c24fe10",
   "metadata": {},
   "source": [
    "j) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16f35da6-2f96-448b-8aa1-8f5607c25e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.683752\n",
      "         Iterations 4\n",
      "Confusion Matrix:\n",
      "             Predicted Down  Predicted Up\n",
      "Actual Down               5            92\n",
      "Actual Up                 4           116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sinho\\AppData\\Local\\Temp\\ipykernel_14664\\415320807.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Weekly['Direction_numeric'] = label_encoder.fit_transform(Weekly['Direction'])\n"
     ]
    }
   ],
   "source": [
    "#Add your code here\n",
    "\n",
    "# Convert 'Direction' to numeric format\n",
    "label_encoder = LabelEncoder()\n",
    "Weekly['Direction_numeric'] = label_encoder.fit_transform(Weekly['Direction'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(Weekly, test_size=0.2, random_state=16)\n",
    "\n",
    "# Fit logistic regression model with interaction term on the training data\n",
    "formula = 'Direction_numeric ~ Lag2 * Lag4'\n",
    "log_fit_interaction = sm.Logit.from_formula(formula, data=train_data).fit()\n",
    "\n",
    "# Predict probabilities on the test data\n",
    "log_probs_interaction = log_fit_interaction.predict(test_data)\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1)\n",
    "log_pred = (log_probs_interaction > 0.5).astype(int)\n",
    "\n",
    "# Decode numeric predictions back to original labels\n",
    "log_pred_labels = label_encoder.inverse_transform(log_pred)\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(test_data['Direction'], log_pred_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(pd.DataFrame(cm, columns=['Predicted Down', 'Predicted Up'], index=['Actual Down', 'Actual Up']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af85e247",
   "metadata": {},
   "source": [
    "Additional Practice Questions: \n",
    "- Explain the difference between MLR and OLS to a non-technical audience. \n",
    "- How would you describe this exercise in an interview to both a technical and non-technical interviewer?\n",
    "- What are the key insights you would want to show? \n",
    "- Can you think of a business context where this exercise would have applications?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
